{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "import warnings"
      ],
      "metadata": {
        "id": "IrPvnzWA7EX-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  FIXED: load_and_prepare_data function - Proper indentation & logic\n",
        "\n",
        "def load_and_prepare_data(filepath='cloud_dataset.csv'):\n",
        "    \"\"\"Load ANY CSV and auto-detect telemetry columns - NO DateTime required!\"\"\"\n",
        "    df = pd.read_csv(filepath)\n",
        "    print(\" ACTUAL Columns:\", df.columns.tolist())\n",
        "    print(\"\\n First 3 rows:\")\n",
        "    print(df.head(3))\n",
        "    print(f\"\\nShape: {df.shape}\")\n",
        "\n",
        "    # Auto-detect numeric telemetry columns\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    print(f\"\\n Numeric columns (telemetry): {num_cols}\")\n",
        "\n",
        "    # Create target from anomaly labels OR high resource usage\n",
        "    if 'Label' in df.columns:\n",
        "        print(\" Using 'Label' column for target\")\n",
        "        df['future_outage'] = df['Label'].rolling(window=60, min_periods=1).max().shift(-60).fillna(0).astype(int)\n",
        "    else:\n",
        "        # Fallback: high usage threshold\n",
        "        print(\" Creating synthetic target from high usage\")\n",
        "        high_usage = (df[num_cols].max(axis=1) > df[num_cols].quantile(0.95)).astype(int)\n",
        "        df['future_outage'] = high_usage.rolling(60, min_periods=1).max().shift(-60).fillna(0).astype(int)\n",
        "\n",
        "    print(f\" Outage rate: {df['future_outage'].mean():.1%}\")\n",
        "    return df, num_cols\n"
      ],
      "metadata": {
        "id": "BFM-N_mM7ZMZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(df, base_cols):\n",
        "    \"\"\"Safe feature engineering using ONLY existing columns\"\"\"\n",
        "    print(f\"\\nðŸ”§ Engineering features from: {base_cols[:3]}...\")\n",
        "\n",
        "    # Rolling stats on base telemetry\n",
        "    for i, col in enumerate(base_cols[:4]):  # Limit to top 4 to avoid explosion\n",
        "        df[f'{col}_roll_mean_5'] = df[col].rolling(5, min_periods=1).mean()\n",
        "        df[f'{col}_roll_std_10'] = df[col].rolling(10, min_periods=1).std()\n",
        "\n",
        "    # Simple ratios\n",
        "    if len(base_cols) >= 2:\n",
        "        df['ratio_01'] = df[base_cols[0]] / (df[base_cols[1]] + 1e-6)\n",
        "        df['ratio_sum'] = df[base_cols[:2]].sum(axis=1)\n",
        "\n",
        "    # Encode categorical columns\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in cat_cols:\n",
        "        le = LabelEncoder()\n",
        "        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str).fillna('missing'))\n",
        "\n",
        "    print(\" Features engineered!\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "MPaH-twN74sb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df, target_col='future_outage'):\n",
        "    \"\"\"Robust split + scaling - NO time column needed\"\"\"\n",
        "    # All numeric features except target\n",
        "    feature_cols = [col for col in df.columns\n",
        "                   if col not in [target_col] and df[col].dtype in [np.float64, np.int64]]\n",
        "\n",
        "    print(f\"\\n Using {len(feature_cols)} features\")\n",
        "\n",
        "    # Drop NaNs and split\n",
        "    df_clean = df[feature_cols + [target_col]].dropna()\n",
        "    split_idx = int(0.7 * len(df_clean))\n",
        "\n",
        "    X = df_clean[feature_cols].iloc[:split_idx].values\n",
        "    y = df_clean[target_col].iloc[:split_idx].values\n",
        "    X_test = df_clean[feature_cols].iloc[split_idx:].values\n",
        "    y_test = df_clean[target_col].iloc[split_idx:].values\n",
        "\n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"Train: {len(X_train_scaled)} | Test: {len(X_test_scaled)}\")\n",
        "    print(f\"   Train positive rate: {y.mean():.1%} | Test: {y_test.mean():.1%}\")\n",
        "\n",
        "    return X_train_scaled, y, X_test_scaled, y_test, scaler, feature_cols"
      ],
      "metadata": {
        "id": "txqftsAo8GRK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(model, X_tr, y_tr, X_te, y_te, name):\n",
        "    \"\"\"Train + evaluate with full metrics\"\"\"\n",
        "    model.fit(X_tr, y_tr)\n",
        "    y_pred = model.predict(X_te)\n",
        "    y_proba = model.predict_proba(X_te)[:, 1]\n",
        "\n",
        "    metrics = {\n",
        "        'ROC-AUC': roc_auc_score(y_te, y_proba),\n",
        "        'Precision': precision_score(y_te, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_te, y_pred, zero_division=0),\n",
        "        'F1': f1_score(y_te, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(classification_report(y_te, y_pred, zero_division=0))\n",
        "    return metrics, model\n",
        "\n",
        "# === RUN EVERYTHING ===\n",
        "if __name__ == \"__main__\":\n",
        "    print(\" CLOUD OUTAGE PREDICTOR v2.0 - WORKS WITH ANY DATASET!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # PIPELINE\n",
        "        df, telemetry_cols = load_and_prepare_data('cloud_dataset.csv')\n",
        "        df = engineer_features(df, telemetry_cols)\n",
        "        X_tr, y_tr, X_te, y_te, scaler, feats = preprocess_data(df)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\" TRAINING MODELS...\")\n",
        "\n",
        "        metrics_results = {}\n",
        "        trained_models = {}\n",
        "\n",
        "        # Logistic Regression (baseline)\n",
        "        lr_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "        metrics_results['Logistic'], trained_models['Logistic'] = train_and_eval(\n",
        "            lr_model, X_tr, y_tr, X_te, y_te, \"Logistic Regression\")\n",
        "\n",
        "        # Random Forest\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
        "        metrics_results['Random Forest'], trained_models['Random Forest'] = train_and_eval(\n",
        "            rf_model, X_tr, y_tr, X_te, y_te, \"Random Forest\")\n",
        "\n",
        "        # XGBoost (best for imbalanced DevOps data)\n",
        "        xgb_model = XGBClassifier(n_estimators=100, scale_pos_weight=10, random_state=42)\n",
        "        metrics_results['XGBoost'], trained_models['XGBoost'] = train_and_eval(\n",
        "            xgb_model, X_tr, y_tr, X_te, y_te, \"XGBoost\")\n",
        "\n",
        "        # FINAL COMPARISON\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\" MODEL COMPARISON\")\n",
        "        print(\"=\"*60)\n",
        "        results_df = pd.DataFrame(metrics_results).T.round(3)\n",
        "        print(results_df[['ROC-AUC', 'Precision', 'Recall', 'F1']])\n",
        "\n",
        "        best_model_name = results_df['Recall'].idxmax()\n",
        "        best_recall = results_df['Recall'].max()\n",
        "        print(f\"\\n PRODUCTION PICK: {best_model_name}\")\n",
        "        print(f\"   Recall: {best_recall:.3f} (catches {best_recall:.1%} of outages)\")\n",
        "\n",
        "        # Save the best model and the scaler\n",
        "        import joblib\n",
        "        joblib.dump(scaler, 'scaler.joblib')\n",
        "        joblib.dump(trained_models[best_model_name], f'{best_model_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
        "        print(f\"\\nSaved scaler.joblib and {best_model_name.lower().replace(' ', '_')}_model.joblib\")\n",
        "\n",
        "        print(\"\\n PIPELINE COMPLETE - Deploy Ready!\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n FILE MISSING - Download from:\")\n",
        "        print(\"https://www.kaggle.com/datasets/programmer3/cloud-resource-usage-dataset-for-anomaly-detection\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback for debugging\n",
        "        print(\"This pipeline works with ANY CSV containing numeric telemetry columns!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COQOs4vz8KVF",
        "outputId": "6fe77649-2626-419f-cc63-8ec644592831"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CLOUD OUTAGE PREDICTOR v2.0 - WORKS WITH ANY DATASET!\n",
            "============================================================\n",
            " ACTUAL Columns: ['Timestamp', 'CPU_Usage', 'Memory_Usage', 'Disk_IO', 'Network_IO', 'Workload_Type', 'User_ID', 'Anomaly_Label']\n",
            "\n",
            " First 3 rows:\n",
            "             Timestamp  CPU_Usage  Memory_Usage  Disk_IO  Network_IO  \\\n",
            "0  2025-07-01 00:00:00      18.88         43.19    11.40        6.01   \n",
            "1  2025-07-01 00:01:00      25.31         45.43     7.68       17.67   \n",
            "2  2025-07-01 00:02:00       3.87         49.50    14.08        3.48   \n",
            "\n",
            "     Workload_Type User_ID  Anomaly_Label  \n",
            "0   Database_Query  user_1              0  \n",
            "1  Video_Streaming  user_1              0  \n",
            "2   Database_Query  user_1              0  \n",
            "\n",
            "Shape: (14400, 8)\n",
            " Using 'Anomaly_Label' column for target\n",
            "\n",
            " Numeric columns (telemetry for features): ['CPU_Usage', 'Memory_Usage', 'Disk_IO', 'Network_IO']\n",
            " Outage rate: 98.5%\n",
            "\n",
            "ðŸ”§ Engineering features from: ['CPU_Usage', 'Memory_Usage', 'Disk_IO']...\n",
            " Features engineered!\n",
            "\n",
            " Using 18 features\n",
            "Train: 10079 | Test: 4320\n",
            "   Train positive rate: 98.6% | Test: 98.2%\n",
            "\n",
            "============================================================\n",
            " TRAINING MODELS...\n",
            "\n",
            "Logistic Regression Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.01      0.16      0.01        77\n",
            "           1       0.97      0.56      0.71      4243\n",
            "\n",
            "    accuracy                           0.56      4320\n",
            "   macro avg       0.49      0.36      0.36      4320\n",
            "weighted avg       0.96      0.56      0.70      4320\n",
            "\n",
            "\n",
            "Random Forest Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        77\n",
            "           1       0.98      1.00      0.99      4243\n",
            "\n",
            "    accuracy                           0.98      4320\n",
            "   macro avg       0.49      0.50      0.49      4320\n",
            "weighted avg       0.96      0.98      0.97      4320\n",
            "\n",
            "\n",
            "XGBoost Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        77\n",
            "           1       0.98      0.98      0.98      4243\n",
            "\n",
            "    accuracy                           0.96      4320\n",
            "   macro avg       0.49      0.49      0.49      4320\n",
            "weighted avg       0.96      0.96      0.96      4320\n",
            "\n",
            "\n",
            "============================================================\n",
            " MODEL COMPARISON\n",
            "============================================================\n",
            "               ROC-AUC  Precision  Recall     F1\n",
            "Logistic         0.422      0.974   0.564  0.714\n",
            "Random Forest    0.439      0.982   0.998  0.990\n",
            "XGBoost          0.682      0.982   0.979  0.980\n",
            "\n",
            " PRODUCTION PICK: Random Forest\n",
            "   Recall: 0.998 (catches 99.8% of outages)\n",
            "\n",
            "Saved scaler.joblib and random_forest_model.joblib\n",
            "\n",
            " PIPELINE COMPLETE - Deploy Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff6b4e15",
        "outputId": "64b650e6-bb7b-4400-94b2-600e6edff18b"
      },
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n--- Demonstrating Production Prediction Workflow ---\")\n",
        "\n",
        "# 1. Load the saved scaler and model\n",
        "print(\"Loading saved scaler and model...\")\n",
        "loaded_scaler = joblib.load('scaler.joblib')\n",
        "loaded_model = joblib.load('random_forest_model.joblib') # Load the best model identified\n",
        "print(\"Scaler and model loaded successfully.\")\n",
        "\n",
        "\n",
        "num_features_expected = len(loaded_scaler.mean_)\n",
        "\n",
        "\n",
        "\n",
        "synthetic_scaled_new_data = np.array([[-0.1, 0.5, -0.3, 0.2, 0.0, 0.1, -0.2, 0.3, -0.1, 0.0, 0.1, -0.1, 0.2, 0.5, 0.8, -1.0, 0.5, 1.0]])\n",
        "\n",
        "print(f\"\\nSynthetic new scaled data point: {synthetic_scaled_new_data}\")\n",
        "\n",
        "prediction = loaded_model.predict(synthetic_scaled_new_data)\n",
        "probability = loaded_model.predict_proba(synthetic_scaled_new_data)[:, 1]\n",
        "\n",
        "print(f\"\\nPrediction for new data (0=No Outage, 1=Outage): {prediction[0]}\")\n",
        "print(f\"Probability of Outage: {probability[0]:.4f}\")\n",
        "\n",
        "print(\"\\n--- End of Production Prediction Demo ---\")\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Demonstrating Production Prediction Workflow ---\n",
            "Loading saved scaler and model...\n",
            "Scaler and model loaded successfully.\n",
            "\n",
            "Synthetic new scaled data point: [[-0.1  0.5 -0.3  0.2  0.   0.1 -0.2  0.3 -0.1  0.   0.1 -0.1  0.2  0.5\n",
            "   0.8 -1.   0.5  1. ]]\n",
            "\n",
            "Prediction for new data (0=No Outage, 1=Outage): 1\n",
            "Probability of Outage: 1.0000\n",
            "\n",
            "--- End of Production Prediction Demo ---\n"
          ]
        }
      ]
    }
  ]
}